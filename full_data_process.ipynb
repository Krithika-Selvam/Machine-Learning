{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "full data process.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krithika-Selvam/Machine-Learning/blob/main/full_data_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euApKVXu23yA"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0J_ighy8_z9",
        "outputId": "f617c3dc-fca1-4387-c346-1716e14cd59c"
      },
      "source": [
        "df=pd.read_csv('testdata1.csv')\r\n",
        "df.head()\r\n",
        "df.isnull().any() "
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cs41           False\n",
              "cs42           False\n",
              "cs43           False\n",
              "alhpu          False\n",
              "ashpu          False\n",
              "10th           False\n",
              "12th           False\n",
              "att            False\n",
              "fp/aarnps      False\n",
              "travh          False\n",
              "netcon         False\n",
              "rev            False\n",
              "semgpa         False\n",
              "Unnamed: 13     True\n",
              "Unnamed: 14     True\n",
              "Unnamed: 15     True\n",
              "Unnamed: 16     True\n",
              "Unnamed: 17     True\n",
              "Unnamed: 18     True\n",
              "Unnamed: 19     True\n",
              "Unnamed: 20     True\n",
              "Unnamed: 21     True\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry3COm5G9xYf",
        "outputId": "686deee7-46cc-44fe-8783-7bfe373b3613"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "x=df[['cs41','cs42','cs43','12th','att','fp/aarnps','travh','netcon','rev']].values\r\n",
        "y=df['semgpa'].values\r\n",
        "x"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[82.17, 89.  , 85.59, ...,  0.  ,  1.  ,  1.  ],\n",
              "       [35.5 , 40.33, 37.92, ...,  0.  ,  0.  ,  0.  ],\n",
              "       [78.5 , 80.67, 79.59, ...,  0.  ,  1.  ,  0.  ],\n",
              "       ...,\n",
              "       [88.5 , 83.  , 85.75, ...,  0.  ,  0.  ,  1.  ],\n",
              "       [72.83, 74.5 , 73.67, ...,  0.  ,  0.  ,  1.  ],\n",
              "       [72.14,  0.  , 72.14, ...,  0.  ,  1.  ,  0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPRUjPbp_Jg-",
        "outputId": "ac1bb5fb-418d-4caa-cf6e-a334ae8569bb"
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.009,random_state=42)\r\n",
        "#normalize bteween 0 and 1\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "scaler=MinMaxScaler()\r\n",
        "scaler.fit(x_train)\r\n",
        "x_train=scaler.transform(x_train)\r\n",
        "x_test=scaler.transform(x_test)\r\n",
        "x_train\r\n",
        "x_test\r\n"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.43942697, 0.32136683, 0.38015424, ..., 1.        , 1.        ,\n",
              "        0.        ],\n",
              "       [0.7664279 , 0.67965016, 0.72318766, ..., 0.        , 0.        ,\n",
              "        1.        ],\n",
              "       [0.87376726, 0.84745246, 0.86118252, ..., 0.        , 1.        ,\n",
              "        1.        ],\n",
              "       ...,\n",
              "       [0.7664279 , 0.83728262, 0.80287918, ..., 1.        , 0.        ,\n",
              "        1.        ],\n",
              "       [0.8097166 , 0.75256788, 0.781491  , ..., 0.        , 0.        ,\n",
              "        1.        ],\n",
              "       [0.81490709, 0.9254551 , 0.8714653 , ..., 1.        , 1.        ,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCQIlTCzBqaQ"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "model=Sequential()\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "model.add(Dense(10,activation='relu'))\r\n",
        "#model.add(Dense(8,activation='relu'))\r\n",
        "#model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(8,activation='relu'))\r\n",
        "model.add(Dense(1))\r\n",
        "model.compile(optimizer='adam',loss='mse')\r\n"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlYdygViVVQg"
      },
      "source": [
        ""
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P12URm29neCP",
        "outputId": "ef700d68-38c3-46ed-cc10-8fe62ea6bac9"
      },
      "source": [
        "model.fit(x=x_train,y=y_train,epochs=250,validation_split=0.2)#one pass over the dataset"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 2146.7126 - val_loss: 61.3180\n",
            "Epoch 2/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 60.6944 - val_loss: 57.3407\n",
            "Epoch 3/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 51.7526 - val_loss: 50.4626\n",
            "Epoch 4/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 44.5169 - val_loss: 42.7559\n",
            "Epoch 5/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 41.9767 - val_loss: 47.7914\n",
            "Epoch 6/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 41.8745 - val_loss: 42.8759\n",
            "Epoch 7/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 39.8288 - val_loss: 40.1359\n",
            "Epoch 8/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 38.8595 - val_loss: 40.6054\n",
            "Epoch 9/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 38.7551 - val_loss: 38.8160\n",
            "Epoch 10/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 37.6777 - val_loss: 37.0575\n",
            "Epoch 11/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 37.4527 - val_loss: 36.7276\n",
            "Epoch 12/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 37.1899 - val_loss: 45.1664\n",
            "Epoch 13/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 38.3899 - val_loss: 37.6078\n",
            "Epoch 14/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 36.4718 - val_loss: 41.6305\n",
            "Epoch 15/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 36.0778 - val_loss: 38.3862\n",
            "Epoch 16/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 36.8939 - val_loss: 36.5267\n",
            "Epoch 17/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 35.4371 - val_loss: 40.0273\n",
            "Epoch 18/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 34.5461 - val_loss: 34.7974\n",
            "Epoch 19/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 34.7669 - val_loss: 40.7057\n",
            "Epoch 20/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 35.3692 - val_loss: 33.9428\n",
            "Epoch 21/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 33.2885 - val_loss: 33.5089\n",
            "Epoch 22/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 34.3290 - val_loss: 32.6603\n",
            "Epoch 23/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 33.5411 - val_loss: 33.4503\n",
            "Epoch 24/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 33.2955 - val_loss: 39.0637\n",
            "Epoch 25/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 34.1292 - val_loss: 32.8638\n",
            "Epoch 26/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 34.7199 - val_loss: 33.2394\n",
            "Epoch 27/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 33.9129 - val_loss: 32.3690\n",
            "Epoch 28/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 32.7845 - val_loss: 33.7490\n",
            "Epoch 29/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 32.3819 - val_loss: 31.4259\n",
            "Epoch 30/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 31.1878 - val_loss: 31.7383\n",
            "Epoch 31/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 31.6470 - val_loss: 32.0751\n",
            "Epoch 32/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 31.8425 - val_loss: 33.0794\n",
            "Epoch 33/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 30.9607 - val_loss: 31.3251\n",
            "Epoch 34/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 30.3532 - val_loss: 41.8046\n",
            "Epoch 35/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 29.7497 - val_loss: 29.1946\n",
            "Epoch 36/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 29.0947 - val_loss: 31.1782\n",
            "Epoch 37/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 29.7527 - val_loss: 29.5455\n",
            "Epoch 38/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 29.2853 - val_loss: 29.4697\n",
            "Epoch 39/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 28.5680 - val_loss: 27.6917\n",
            "Epoch 40/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 28.5863 - val_loss: 27.6140\n",
            "Epoch 41/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 28.4092 - val_loss: 27.5368\n",
            "Epoch 42/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 28.3578 - val_loss: 26.2893\n",
            "Epoch 43/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 27.5453 - val_loss: 26.7556\n",
            "Epoch 44/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 28.0948 - val_loss: 26.9869\n",
            "Epoch 45/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 28.6834 - val_loss: 26.5567\n",
            "Epoch 46/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 27.4140 - val_loss: 27.7114\n",
            "Epoch 47/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 26.9771 - val_loss: 26.5316\n",
            "Epoch 48/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 26.4655 - val_loss: 25.4657\n",
            "Epoch 49/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 26.6994 - val_loss: 27.3939\n",
            "Epoch 50/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 27.4250 - val_loss: 26.9920\n",
            "Epoch 51/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 24.7060 - val_loss: 26.2124\n",
            "Epoch 52/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 26.8637 - val_loss: 26.8855\n",
            "Epoch 53/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 25.9947 - val_loss: 24.4183\n",
            "Epoch 54/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 26.4250 - val_loss: 25.1752\n",
            "Epoch 55/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 25.2489 - val_loss: 23.2083\n",
            "Epoch 56/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 24.8128 - val_loss: 29.9972\n",
            "Epoch 57/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 23.9289 - val_loss: 25.0480\n",
            "Epoch 58/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 24.2631 - val_loss: 23.8463\n",
            "Epoch 59/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 24.3050 - val_loss: 23.3239\n",
            "Epoch 60/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 23.7580 - val_loss: 24.0188\n",
            "Epoch 61/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 23.9380 - val_loss: 27.8327\n",
            "Epoch 62/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 23.4081 - val_loss: 21.9540\n",
            "Epoch 63/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 22.9607 - val_loss: 24.3348\n",
            "Epoch 64/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 22.3954 - val_loss: 27.0891\n",
            "Epoch 65/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 23.4101 - val_loss: 21.5178\n",
            "Epoch 66/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 21.8699 - val_loss: 26.9184\n",
            "Epoch 67/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 22.0843 - val_loss: 20.1213\n",
            "Epoch 68/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 21.5325 - val_loss: 21.4728\n",
            "Epoch 69/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 21.1902 - val_loss: 20.3928\n",
            "Epoch 70/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 21.2974 - val_loss: 22.3136\n",
            "Epoch 71/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 20.6303 - val_loss: 20.2850\n",
            "Epoch 72/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 21.0337 - val_loss: 24.8599\n",
            "Epoch 73/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 20.8260 - val_loss: 25.7177\n",
            "Epoch 74/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 20.4598 - val_loss: 19.4594\n",
            "Epoch 75/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 21.0222 - val_loss: 20.7806\n",
            "Epoch 76/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.7905 - val_loss: 22.1148\n",
            "Epoch 77/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 20.5601 - val_loss: 19.5287\n",
            "Epoch 78/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 20.8782 - val_loss: 20.4214\n",
            "Epoch 79/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.4487 - val_loss: 19.2797\n",
            "Epoch 80/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.9227 - val_loss: 18.8947\n",
            "Epoch 81/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.5446 - val_loss: 22.9442\n",
            "Epoch 82/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 20.6346 - val_loss: 18.7567\n",
            "Epoch 83/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.4191 - val_loss: 18.9674\n",
            "Epoch 84/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 19.2185 - val_loss: 20.6286\n",
            "Epoch 85/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 20.1775 - val_loss: 19.2631\n",
            "Epoch 86/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.9933 - val_loss: 18.3825\n",
            "Epoch 87/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.8065 - val_loss: 17.7287\n",
            "Epoch 88/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 18.8049 - val_loss: 18.4133\n",
            "Epoch 89/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.6778 - val_loss: 20.5100\n",
            "Epoch 90/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.7003 - val_loss: 17.6775\n",
            "Epoch 91/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.0469 - val_loss: 17.7166\n",
            "Epoch 92/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 18.4764 - val_loss: 17.6412\n",
            "Epoch 93/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 18.2250 - val_loss: 17.4583\n",
            "Epoch 94/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 18.2047 - val_loss: 22.1291\n",
            "Epoch 95/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 18.8018 - val_loss: 17.7403\n",
            "Epoch 96/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.1072 - val_loss: 18.1911\n",
            "Epoch 97/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 17.8736 - val_loss: 18.7309\n",
            "Epoch 98/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 18.8280 - val_loss: 20.3754\n",
            "Epoch 99/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 19.1747 - val_loss: 28.0430\n",
            "Epoch 100/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 19.7078 - val_loss: 19.6581\n",
            "Epoch 101/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 18.6585 - val_loss: 17.5913\n",
            "Epoch 102/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 18.2745 - val_loss: 18.2335\n",
            "Epoch 103/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 17.6721 - val_loss: 18.8016\n",
            "Epoch 104/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 17.4964 - val_loss: 17.3335\n",
            "Epoch 105/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 18.2009 - val_loss: 18.6029\n",
            "Epoch 106/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 17.3980 - val_loss: 16.9410\n",
            "Epoch 107/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 17.5813 - val_loss: 16.5257\n",
            "Epoch 108/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 17.2642 - val_loss: 17.0238\n",
            "Epoch 109/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 17.4199 - val_loss: 17.7778\n",
            "Epoch 110/250\n",
            "633/633 [==============================] - 2s 3ms/step - loss: 16.9278 - val_loss: 22.7510\n",
            "Epoch 111/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.8341 - val_loss: 16.7370\n",
            "Epoch 112/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 17.2456 - val_loss: 16.1356\n",
            "Epoch 113/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.7981 - val_loss: 15.7787\n",
            "Epoch 114/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.7538 - val_loss: 15.9576\n",
            "Epoch 115/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.2218 - val_loss: 18.3591\n",
            "Epoch 116/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 17.0617 - val_loss: 16.0044\n",
            "Epoch 117/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.7769 - val_loss: 17.2150\n",
            "Epoch 118/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 17.3985 - val_loss: 19.2051\n",
            "Epoch 119/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.9842 - val_loss: 15.0147\n",
            "Epoch 120/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.8197 - val_loss: 15.3644\n",
            "Epoch 121/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.0874 - val_loss: 15.4105\n",
            "Epoch 122/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 17.0056 - val_loss: 16.0560\n",
            "Epoch 123/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 17.1165 - val_loss: 22.7811\n",
            "Epoch 124/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.2625 - val_loss: 18.5198\n",
            "Epoch 125/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.9730 - val_loss: 19.7289\n",
            "Epoch 126/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.4405 - val_loss: 20.0790\n",
            "Epoch 127/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.6003 - val_loss: 14.9884\n",
            "Epoch 128/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.7981 - val_loss: 16.7797\n",
            "Epoch 129/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.8870 - val_loss: 14.7938\n",
            "Epoch 130/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.6591 - val_loss: 15.3241\n",
            "Epoch 131/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.3712 - val_loss: 14.7678\n",
            "Epoch 132/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.4353 - val_loss: 16.2296\n",
            "Epoch 133/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.1907 - val_loss: 20.2324\n",
            "Epoch 134/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.6417 - val_loss: 14.5595\n",
            "Epoch 135/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.4520 - val_loss: 18.3967\n",
            "Epoch 136/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.8856 - val_loss: 19.1495\n",
            "Epoch 137/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 15.5775 - val_loss: 16.6177\n",
            "Epoch 138/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.6495 - val_loss: 13.6799\n",
            "Epoch 139/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.9385 - val_loss: 16.0724\n",
            "Epoch 140/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.2113 - val_loss: 14.2041\n",
            "Epoch 141/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.0590 - val_loss: 13.5944\n",
            "Epoch 142/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.2161 - val_loss: 14.8847\n",
            "Epoch 143/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.7565 - val_loss: 13.6772\n",
            "Epoch 144/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.7139 - val_loss: 13.9499\n",
            "Epoch 145/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.7609 - val_loss: 13.9599\n",
            "Epoch 146/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.6835 - val_loss: 13.7122\n",
            "Epoch 147/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.5309 - val_loss: 13.4991\n",
            "Epoch 148/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.8381 - val_loss: 13.8225\n",
            "Epoch 149/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.2756 - val_loss: 14.1806\n",
            "Epoch 150/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 15.1407 - val_loss: 13.2450\n",
            "Epoch 151/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.1079 - val_loss: 13.6381\n",
            "Epoch 152/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.3230 - val_loss: 28.4532\n",
            "Epoch 153/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 16.4048 - val_loss: 14.2295\n",
            "Epoch 154/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.3374 - val_loss: 14.3957\n",
            "Epoch 155/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.1573 - val_loss: 13.4186\n",
            "Epoch 156/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.6417 - val_loss: 14.5499\n",
            "Epoch 157/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.3357 - val_loss: 14.4940\n",
            "Epoch 158/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.7209 - val_loss: 18.9715\n",
            "Epoch 159/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.4700 - val_loss: 13.5660\n",
            "Epoch 160/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.0801 - val_loss: 13.9494\n",
            "Epoch 161/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 15.1717 - val_loss: 13.2387\n",
            "Epoch 162/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.9190 - val_loss: 12.9735\n",
            "Epoch 163/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.6026 - val_loss: 13.4422\n",
            "Epoch 164/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.2258 - val_loss: 13.6872\n",
            "Epoch 165/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.3393 - val_loss: 13.6421\n",
            "Epoch 166/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.0720 - val_loss: 18.1778\n",
            "Epoch 167/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.7677 - val_loss: 14.9918\n",
            "Epoch 168/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.5444 - val_loss: 12.9097\n",
            "Epoch 169/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.1060 - val_loss: 14.3573\n",
            "Epoch 170/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.9797 - val_loss: 15.4263\n",
            "Epoch 171/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.8419 - val_loss: 13.7366\n",
            "Epoch 172/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.0866 - val_loss: 18.0458\n",
            "Epoch 173/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.0270 - val_loss: 13.4323\n",
            "Epoch 174/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.8436 - val_loss: 12.5455\n",
            "Epoch 175/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.5408 - val_loss: 13.5530\n",
            "Epoch 176/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.7315 - val_loss: 13.8366\n",
            "Epoch 177/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 13.9444 - val_loss: 13.7122\n",
            "Epoch 178/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.7697 - val_loss: 14.3607\n",
            "Epoch 179/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.0484 - val_loss: 14.9340\n",
            "Epoch 180/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.1597 - val_loss: 14.2053\n",
            "Epoch 181/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.7846 - val_loss: 12.7962\n",
            "Epoch 182/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.6446 - val_loss: 14.1853\n",
            "Epoch 183/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.6370 - val_loss: 12.9998\n",
            "Epoch 184/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.7644 - val_loss: 14.3215\n",
            "Epoch 185/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.6413 - val_loss: 12.6887\n",
            "Epoch 186/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.1852 - val_loss: 13.8604\n",
            "Epoch 187/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.8058 - val_loss: 13.2657\n",
            "Epoch 188/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.1243 - val_loss: 19.0882\n",
            "Epoch 189/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.8816 - val_loss: 20.0274\n",
            "Epoch 190/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.4844 - val_loss: 12.3770\n",
            "Epoch 191/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.5660 - val_loss: 14.3009\n",
            "Epoch 192/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.5708 - val_loss: 15.0509\n",
            "Epoch 193/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.1252 - val_loss: 12.0875\n",
            "Epoch 194/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 13.8597 - val_loss: 13.9374\n",
            "Epoch 195/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 13.5657 - val_loss: 15.3871\n",
            "Epoch 196/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.4043 - val_loss: 12.3168\n",
            "Epoch 197/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.9297 - val_loss: 15.9735\n",
            "Epoch 198/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 13.9106 - val_loss: 12.6240\n",
            "Epoch 199/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.4897 - val_loss: 12.6958\n",
            "Epoch 200/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.8194 - val_loss: 14.1844\n",
            "Epoch 201/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.7489 - val_loss: 13.4057\n",
            "Epoch 202/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.3285 - val_loss: 12.6656\n",
            "Epoch 203/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 13.8183 - val_loss: 12.3157\n",
            "Epoch 204/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.2714 - val_loss: 13.6760\n",
            "Epoch 205/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.2089 - val_loss: 12.3118\n",
            "Epoch 206/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 13.2729 - val_loss: 12.9991\n",
            "Epoch 207/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.2812 - val_loss: 12.4095\n",
            "Epoch 208/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.0257 - val_loss: 13.7615\n",
            "Epoch 209/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.1723 - val_loss: 22.5334\n",
            "Epoch 210/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.9238 - val_loss: 16.4349\n",
            "Epoch 211/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.3748 - val_loss: 11.7802\n",
            "Epoch 212/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.5505 - val_loss: 14.4636\n",
            "Epoch 213/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 14.0819 - val_loss: 13.0619\n",
            "Epoch 214/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.1247 - val_loss: 16.4912\n",
            "Epoch 215/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.0624 - val_loss: 19.7931\n",
            "Epoch 216/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 14.2623 - val_loss: 12.2891\n",
            "Epoch 217/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 14.1261 - val_loss: 12.1557\n",
            "Epoch 218/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.3663 - val_loss: 11.9890\n",
            "Epoch 219/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.8490 - val_loss: 11.8378\n",
            "Epoch 220/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.3034 - val_loss: 12.6062\n",
            "Epoch 221/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.1104 - val_loss: 13.7867\n",
            "Epoch 222/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.9887 - val_loss: 12.2180\n",
            "Epoch 223/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.6093 - val_loss: 13.0165\n",
            "Epoch 224/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.3316 - val_loss: 13.3217\n",
            "Epoch 225/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.1854 - val_loss: 11.5923\n",
            "Epoch 226/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.6602 - val_loss: 16.7862\n",
            "Epoch 227/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.8390 - val_loss: 13.3233\n",
            "Epoch 228/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.1376 - val_loss: 14.3819\n",
            "Epoch 229/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.4909 - val_loss: 17.6227\n",
            "Epoch 230/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.5215 - val_loss: 13.0265\n",
            "Epoch 231/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.7077 - val_loss: 11.4804\n",
            "Epoch 232/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.7525 - val_loss: 16.6314\n",
            "Epoch 233/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.8707 - val_loss: 12.9154\n",
            "Epoch 234/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.1521 - val_loss: 14.4994\n",
            "Epoch 235/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.6749 - val_loss: 12.9011\n",
            "Epoch 236/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.3453 - val_loss: 15.8011\n",
            "Epoch 237/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.2279 - val_loss: 12.2628\n",
            "Epoch 238/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 12.6393 - val_loss: 15.6162\n",
            "Epoch 239/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.1733 - val_loss: 13.7222\n",
            "Epoch 240/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 13.7472 - val_loss: 12.2102\n",
            "Epoch 241/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.6758 - val_loss: 12.9683\n",
            "Epoch 242/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 13.0664 - val_loss: 11.5930\n",
            "Epoch 243/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 12.8891 - val_loss: 12.5516\n",
            "Epoch 244/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.9891 - val_loss: 11.8316\n",
            "Epoch 245/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.6194 - val_loss: 12.2278\n",
            "Epoch 246/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 12.6110 - val_loss: 13.1573\n",
            "Epoch 247/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.9818 - val_loss: 13.1694\n",
            "Epoch 248/250\n",
            "633/633 [==============================] - 2s 2ms/step - loss: 12.9294 - val_loss: 11.6585\n",
            "Epoch 249/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.5763 - val_loss: 11.4289\n",
            "Epoch 250/250\n",
            "633/633 [==============================] - 1s 2ms/step - loss: 12.4302 - val_loss: 11.5328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8850b78f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP-xu14eGTja",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "29aaaf6c-cb9d-41f6-cb63-c458b80df764"
      },
      "source": [
        "pd.DataFrame(model.history.history)"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>758.512939</td>\n",
              "      <td>61.318012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58.800995</td>\n",
              "      <td>57.340729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49.453209</td>\n",
              "      <td>50.462578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43.672321</td>\n",
              "      <td>42.755890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>42.096333</td>\n",
              "      <td>47.791405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>12.775047</td>\n",
              "      <td>13.157259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>12.342156</td>\n",
              "      <td>13.169422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>12.929530</td>\n",
              "      <td>11.658471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>12.613048</td>\n",
              "      <td>11.428949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>12.552985</td>\n",
              "      <td>11.532820</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           loss   val_loss\n",
              "0    758.512939  61.318012\n",
              "1     58.800995  57.340729\n",
              "2     49.453209  50.462578\n",
              "3     43.672321  42.755890\n",
              "4     42.096333  47.791405\n",
              "..          ...        ...\n",
              "245   12.775047  13.157259\n",
              "246   12.342156  13.169422\n",
              "247   12.929530  11.658471\n",
              "248   12.613048  11.428949\n",
              "249   12.552985  11.532820\n",
              "\n",
              "[250 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm2S07ZYGplb",
        "outputId": "91c51b6e-a970-4ef7-bb22-f02c4ae34e64"
      },
      "source": [
        "model.evaluate(x_test,y_test,verbose=0)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.632031440734863"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dg6rN5AHLlJ",
        "outputId": "8d77b052-7306-4e34-9dcb-b3bb617343ec"
      },
      "source": [
        "model.evaluate(x_train,y_train,verbose=0)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.57288932800293"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrlEzctuH5gr",
        "outputId": "4cf3f79c-4e29-4c2c-8b2f-5e23acfdf7d7"
      },
      "source": [
        "test_predictions=model.predict(x_test)\r\n",
        "test_predictions"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[37.582012],\n",
              "       [71.28223 ],\n",
              "       [82.704735],\n",
              "       [90.0647  ],\n",
              "       [73.75593 ],\n",
              "       [75.45652 ],\n",
              "       [89.18362 ],\n",
              "       [74.13223 ],\n",
              "       [79.06832 ],\n",
              "       [66.219406],\n",
              "       [73.66265 ],\n",
              "       [68.66762 ],\n",
              "       [76.02906 ],\n",
              "       [74.75957 ],\n",
              "       [85.412796],\n",
              "       [57.54115 ],\n",
              "       [76.02801 ],\n",
              "       [66.491585],\n",
              "       [78.90931 ],\n",
              "       [60.140987],\n",
              "       [70.81266 ],\n",
              "       [86.459435],\n",
              "       [72.3541  ],\n",
              "       [64.4137  ],\n",
              "       [78.20411 ],\n",
              "       [19.036116],\n",
              "       [82.313385],\n",
              "       [69.27754 ],\n",
              "       [92.144135],\n",
              "       [76.357506],\n",
              "       [74.621475],\n",
              "       [67.25148 ],\n",
              "       [81.184395],\n",
              "       [63.715748],\n",
              "       [74.717766],\n",
              "       [87.16746 ],\n",
              "       [19.036116],\n",
              "       [79.10786 ],\n",
              "       [85.412796],\n",
              "       [66.62005 ],\n",
              "       [82.6208  ],\n",
              "       [83.40343 ],\n",
              "       [76.357506],\n",
              "       [67.67273 ],\n",
              "       [71.76032 ],\n",
              "       [77.42922 ],\n",
              "       [81.702576],\n",
              "       [73.12334 ],\n",
              "       [77.86296 ],\n",
              "       [73.08141 ],\n",
              "       [69.83097 ],\n",
              "       [64.57295 ],\n",
              "       [68.85669 ],\n",
              "       [76.738556],\n",
              "       [75.41564 ],\n",
              "       [78.08902 ],\n",
              "       [74.78426 ],\n",
              "       [85.6614  ],\n",
              "       [60.140987],\n",
              "       [73.72332 ],\n",
              "       [51.386654],\n",
              "       [78.70668 ],\n",
              "       [57.54115 ],\n",
              "       [76.35659 ],\n",
              "       [67.55438 ],\n",
              "       [78.27287 ],\n",
              "       [74.52594 ],\n",
              "       [74.225876],\n",
              "       [78.12117 ],\n",
              "       [75.13735 ],\n",
              "       [77.41085 ],\n",
              "       [72.67238 ],\n",
              "       [74.569756],\n",
              "       [72.488335],\n",
              "       [85.08882 ],\n",
              "       [78.528915],\n",
              "       [74.75957 ],\n",
              "       [75.13735 ],\n",
              "       [78.12117 ],\n",
              "       [71.69143 ],\n",
              "       [81.637276],\n",
              "       [67.472916],\n",
              "       [77.679596],\n",
              "       [79.17559 ],\n",
              "       [79.06832 ],\n",
              "       [76.45873 ],\n",
              "       [92.19103 ],\n",
              "       [78.55335 ],\n",
              "       [89.20537 ],\n",
              "       [80.94161 ],\n",
              "       [76.69947 ],\n",
              "       [79.861336],\n",
              "       [93.1876  ],\n",
              "       [76.10451 ],\n",
              "       [87.335915],\n",
              "       [72.31576 ],\n",
              "       [77.66699 ],\n",
              "       [63.715748],\n",
              "       [75.244026],\n",
              "       [70.09826 ],\n",
              "       [84.33785 ],\n",
              "       [82.25204 ],\n",
              "       [52.76637 ],\n",
              "       [67.55438 ],\n",
              "       [63.908905],\n",
              "       [73.10163 ],\n",
              "       [77.41085 ],\n",
              "       [87.27132 ],\n",
              "       [76.4301  ],\n",
              "       [70.81266 ],\n",
              "       [21.529713],\n",
              "       [74.621475],\n",
              "       [75.04174 ],\n",
              "       [76.25679 ],\n",
              "       [76.635765],\n",
              "       [76.15458 ],\n",
              "       [88.64387 ],\n",
              "       [71.42795 ],\n",
              "       [72.33065 ],\n",
              "       [64.656204],\n",
              "       [52.598812],\n",
              "       [78.43549 ],\n",
              "       [58.425762],\n",
              "       [83.27198 ],\n",
              "       [65.34941 ],\n",
              "       [77.86296 ],\n",
              "       [77.57452 ],\n",
              "       [74.38262 ],\n",
              "       [70.41989 ],\n",
              "       [48.557384],\n",
              "       [84.993385],\n",
              "       [78.07441 ],\n",
              "       [85.412796],\n",
              "       [64.468666],\n",
              "       [76.02906 ],\n",
              "       [78.29264 ],\n",
              "       [63.8016  ],\n",
              "       [82.61211 ],\n",
              "       [57.37231 ],\n",
              "       [74.32353 ],\n",
              "       [26.631601],\n",
              "       [75.39222 ],\n",
              "       [74.70559 ],\n",
              "       [85.08882 ],\n",
              "       [81.66054 ],\n",
              "       [75.14785 ],\n",
              "       [69.83097 ],\n",
              "       [64.64077 ],\n",
              "       [68.50769 ],\n",
              "       [71.073616],\n",
              "       [91.936104],\n",
              "       [80.91566 ],\n",
              "       [61.194496],\n",
              "       [76.471855],\n",
              "       [67.51635 ],\n",
              "       [79.301155],\n",
              "       [70.09826 ],\n",
              "       [76.29141 ],\n",
              "       [75.11757 ],\n",
              "       [70.58164 ],\n",
              "       [87.65772 ],\n",
              "       [78.39306 ],\n",
              "       [62.622032],\n",
              "       [76.357506],\n",
              "       [75.244026],\n",
              "       [76.561   ],\n",
              "       [83.556206],\n",
              "       [76.04885 ],\n",
              "       [67.78724 ],\n",
              "       [82.666885],\n",
              "       [66.436134],\n",
              "       [64.38037 ],\n",
              "       [79.26142 ],\n",
              "       [87.26614 ],\n",
              "       [74.621475],\n",
              "       [73.704506],\n",
              "       [82.26071 ],\n",
              "       [72.839455],\n",
              "       [70.81266 ],\n",
              "       [75.81025 ],\n",
              "       [70.81266 ],\n",
              "       [72.51536 ],\n",
              "       [71.18206 ],\n",
              "       [76.14502 ],\n",
              "       [37.582012],\n",
              "       [69.36543 ],\n",
              "       [79.30183 ],\n",
              "       [84.33785 ],\n",
              "       [79.27651 ],\n",
              "       [26.631601],\n",
              "       [85.124374],\n",
              "       [83.6325  ],\n",
              "       [60.003925],\n",
              "       [33.607647],\n",
              "       [78.226654],\n",
              "       [81.89733 ],\n",
              "       [78.605   ],\n",
              "       [71.73031 ],\n",
              "       [70.674805],\n",
              "       [73.157646],\n",
              "       [74.225876],\n",
              "       [71.37366 ],\n",
              "       [74.0818  ],\n",
              "       [70.252335],\n",
              "       [14.72404 ],\n",
              "       [85.45434 ],\n",
              "       [76.69947 ],\n",
              "       [76.37297 ],\n",
              "       [81.86361 ],\n",
              "       [68.50769 ],\n",
              "       [70.7394  ],\n",
              "       [74.13223 ],\n",
              "       [66.219406],\n",
              "       [75.163895],\n",
              "       [75.10608 ],\n",
              "       [87.881714],\n",
              "       [74.91154 ],\n",
              "       [77.15423 ],\n",
              "       [76.42318 ],\n",
              "       [72.1224  ],\n",
              "       [80.812   ],\n",
              "       [71.05267 ],\n",
              "       [87.55582 ],\n",
              "       [77.38937 ],\n",
              "       [69.52641 ],\n",
              "       [81.87339 ],\n",
              "       [67.68829 ],\n",
              "       [75.07816 ],\n",
              "       [73.157646],\n",
              "       [81.86361 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecun1XsdIPNf",
        "outputId": "7af74722-a819-41f5-cab8-b9372d90d146"
      },
      "source": [
        "test_predictions=pd.Series(test_predictions.reshape(230,))\r\n",
        "test_predictions"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      37.582012\n",
              "1      71.282227\n",
              "2      82.704735\n",
              "3      90.064697\n",
              "4      73.755928\n",
              "         ...    \n",
              "225    81.873390\n",
              "226    67.688293\n",
              "227    75.078163\n",
              "228    73.157646\n",
              "229    81.863609\n",
              "Length: 230, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Nx8i-cGII0Uz",
        "outputId": "188869fc-8490-47dc-cffd-c670b3f03b44"
      },
      "source": [
        "pred_df=pd.DataFrame(y_test,columns=['Prediction Values'])\r\n",
        "pred_df=pd.concat([pred_df,test_predictions],axis=1)\r\n",
        "pred_df.columns=['test true Y','model predictions']\r\n",
        "pred_df"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test true Y</th>\n",
              "      <th>model predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>36.97</td>\n",
              "      <td>37.582012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>66.20</td>\n",
              "      <td>71.282227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>83.75</td>\n",
              "      <td>82.704735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>89.25</td>\n",
              "      <td>90.064697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>71.32</td>\n",
              "      <td>73.755928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>77.00</td>\n",
              "      <td>81.873390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>67.34</td>\n",
              "      <td>67.688293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>71.25</td>\n",
              "      <td>75.078163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>67.50</td>\n",
              "      <td>73.157646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>84.75</td>\n",
              "      <td>81.863609</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>230 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     test true Y  model predictions\n",
              "0          36.97          37.582012\n",
              "1          66.20          71.282227\n",
              "2          83.75          82.704735\n",
              "3          89.25          90.064697\n",
              "4          71.32          73.755928\n",
              "..           ...                ...\n",
              "225        77.00          81.873390\n",
              "226        67.34          67.688293\n",
              "227        71.25          75.078163\n",
              "228        67.50          73.157646\n",
              "229        84.75          81.863609\n",
              "\n",
              "[230 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym3MaWoqLi_l",
        "outputId": "57dda9e6-9751-42a3-e425-c81da13d552e"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error,mean_squared_error\r\n",
        "mean_absolute_error(pred_df['test true Y'],pred_df['model predictions'])"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5745121478205144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNmtMrGV72-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e184c9d-f949-4db1-b8f4-fe3bd229520a"
      },
      "source": [
        "import sklearn.metrics as sm\r\n",
        "print(\"Mean absolute error =\", round(sm.mean_absolute_error(pred_df['test true Y'],pred_df['model predictions']), 2)) \r\n",
        "print(\"Mean squared error =\", round(sm.mean_squared_error(pred_df['test true Y'],pred_df['model predictions']), 2)) \r\n",
        "print(\"Median absolute error =\", round(sm.median_absolute_error(pred_df['test true Y'],pred_df['model predictions']), 2)) \r\n",
        "print(\"Explain variance score =\", round(sm.explained_variance_score(pred_df['test true Y'],pred_df['model predictions']), 2)) \r\n",
        "print(\"R2 score =\", round(sm.r2_score(pred_df['test true Y'],pred_df['model predictions']), 2))"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean absolute error = 2.57\n",
            "Mean squared error = 13.63\n",
            "Median absolute error = 1.85\n",
            "Explain variance score = 0.92\n",
            "R2 score = 0.92\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}